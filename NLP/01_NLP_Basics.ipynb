{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/misrapk/Complete-Data-Science-and-ML-Preparation/blob/master/01_NLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CckGBs9UH2h",
        "outputId": "19ae8b85-024f-40a9-fee8-709f1b1225d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\pkmis\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.5/1.5 MB 18.3 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   ---------------------------------------- 0/5 [tqdm]\n",
            "   -------- ------------------------------- 1/5 [regex]\n",
            "   -------- ------------------------------- 1/5 [regex]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ---------------- ----------------------- 2/5 [joblib]\n",
            "   ------------------------ --------------- 3/5 [click]\n",
            "   ------------------------ --------------- 3/5 [click]\n",
            "   ------------------------ --------------- 3/5 [click]\n",
            "   ------------------------ --------------- 3/5 [click]\n",
            "   ------------------------ --------------- 3/5 [click]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   -------------------------------- ------- 4/5 [nltk]\n",
            "   ---------------------------------------- 5/5 [nltk]\n",
            "\n",
            "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4NAirSS602e",
        "outputId": "32d85f08-b592-4f3a-f968-8760d06c1c8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oocMGJhNUWAU"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Hello and welc.ome to yet another video. This is your programming\n",
        "friend, Peeyush. And this is NLP Playlist. What's your plan?\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fBG65vvuU42H",
        "outputId": "4fc3e770-01c1-4a10-b96a-5735395a4ea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello and welc.ome to yet another video. This is your programming\\nfriend, Peeyush. And this is NLP Playlist. What's your plan?\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DJvaIgLEU5dy"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SdBlnBzHVIhj"
      },
      "outputs": [],
      "source": [
        "documents = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zba23TvPVcZC",
        "outputId": "b814a6af-c8a7-4d3c-a41c-cdf87c04d074"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello and welc.ome to yet another video.',\n",
              " 'This is your programming\\nfriend, Peeyush.',\n",
              " 'And this is NLP Playlist.',\n",
              " \"What's your plan?\"]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohNFb1XYVKYB",
        "outputId": "7e55abdf-6ee2-4de8-d907-997ff0c8d8e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'and',\n",
              " 'welc.ome',\n",
              " 'to',\n",
              " 'yet',\n",
              " 'another',\n",
              " 'video',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'your',\n",
              " 'programming',\n",
              " 'friend',\n",
              " ',',\n",
              " 'Peeyush',\n",
              " '.',\n",
              " 'And',\n",
              " 'this',\n",
              " 'is',\n",
              " 'NLP',\n",
              " 'Playlist',\n",
              " '.',\n",
              " 'What',\n",
              " \"'s\",\n",
              " 'your',\n",
              " 'plan',\n",
              " '?']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mHKhmt6WJrY",
        "outputId": "c0288e37-2a6b-4bd8-eb3c-d1f8f3d41c69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'and',\n",
              " 'welc',\n",
              " '.',\n",
              " 'ome',\n",
              " 'to',\n",
              " 'yet',\n",
              " 'another',\n",
              " 'video',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'your',\n",
              " 'programming',\n",
              " 'friend',\n",
              " ',',\n",
              " 'Peeyush',\n",
              " '.',\n",
              " 'And',\n",
              " 'this',\n",
              " 'is',\n",
              " 'NLP',\n",
              " 'Playlist',\n",
              " '.',\n",
              " 'What',\n",
              " \"'\",\n",
              " 's',\n",
              " 'your',\n",
              " 'plan',\n",
              " '?']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#wordpunct\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Bs1huJ66Ql"
      },
      "source": [
        "## TreenBankWordTokernizer\n",
        "\n",
        "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK7IlpDC6-oV",
        "outputId": "cc07bb42-81cb-4639-eefc-ee4add720d36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'and',\n",
              " 'welc.ome',\n",
              " 'to',\n",
              " 'yet',\n",
              " 'another',\n",
              " 'video.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'your',\n",
              " 'programming',\n",
              " 'friend',\n",
              " ',',\n",
              " 'Peeyush.',\n",
              " 'And',\n",
              " 'this',\n",
              " 'is',\n",
              " 'NLP',\n",
              " 'Playlist.',\n",
              " 'What',\n",
              " \"'s\",\n",
              " 'your',\n",
              " 'plan',\n",
              " '?']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cLhmh5J7qCD"
      },
      "source": [
        "# Stemming\n",
        "The process of reducing a word to its word stem.\n",
        "\n",
        "-- Eating, Eaten, eats --> eat\n",
        "\n",
        "-- going, gone, goes --> go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mANOFoWG66Ca",
        "outputId": "59e725bf-902d-4d2f-bf20-23a07a6b34f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating  -->  eat\n",
            "eats  -->  eat\n",
            "eaten  -->  eaten\n",
            "running  -->  run\n",
            "runs  -->  run\n",
            "ran  -->  ran\n",
            "going  -->  go\n",
            "goesdd  -->  goesdd\n",
            "godess  -->  godess\n",
            "history  -->  histori\n",
            "mistory  -->  mistori\n",
            "congratulations  -->  congratul\n"
          ]
        }
      ],
      "source": [
        "#Porterstemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['eating', 'eats', 'eaten', 'running', 'runs', 'ran',\n",
        "         'going', 'goesdd','godess','history', 'mistory', 'congratulations']\n",
        "\n",
        "for word in words:\n",
        "  print(word, ' --> ', stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BCABt_m88S4"
      },
      "source": [
        "one major disadvantages is some words will not get a good meaning after stemming such as 'hisotry'--> 'histori'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ye4rj38u8ex7"
      },
      "outputs": [],
      "source": [
        "#REgexPStemmer Class\n",
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "19yGM-oHW_VV",
        "outputId": "027b2c0e-e88e-4495-ed2c-32232db7993f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'eat'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_stemmer.stem('eating')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U6kwVTin9pW6"
      },
      "outputs": [],
      "source": [
        "  # Lematizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hQ6yT8x1TONx",
        "outputId": "5004c766-d3f6-4687-9228-f70523a4e757"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatizer.lemmatize('going', pos='v') # v = verb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sU5GG-TTR0U",
        "outputId": "5a5885fb-91b5-43ae-f44d-4f2117db2846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating  -->  eating\n",
            "eats  -->  eats\n",
            "eaten  -->  eaten\n",
            "running  -->  running\n",
            "runs  -->  run\n",
            "ran  -->  ran\n",
            "going  -->  going\n",
            "goesdd  -->  goesdd\n",
            "godess  -->  godess\n",
            "history  -->  history\n",
            "mistory  -->  mistory\n",
            "congratulations  -->  congratulation\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word, ' --> ', lemmatizer.lemmatize(word, pos='n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBKb1aU0TxUr",
        "outputId": "bbbd54ab-fb41-4d2d-837c-a55bc8b86c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating  -->  eat\n",
            "eats  -->  eat\n",
            "eaten  -->  eat\n",
            "running  -->  run\n",
            "runs  -->  run\n",
            "ran  -->  run\n",
            "going  -->  go\n",
            "goesdd  -->  goesdd\n",
            "godess  -->  godess\n",
            "history  -->  history\n",
            "mistory  -->  mistory\n",
            "congratulations  -->  congratulations\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word, ' --> ', lemmatizer.lemmatize(word, pos='v'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0Ds6MmlY98-"
      },
      "source": [
        "# Text Preporcessing - Stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C8V7cEbJT1Uj"
      },
      "outputs": [],
      "source": [
        "para = \"\"\"In his legendary 2005 commencement speech at Stanford University, Steve Jobs shared three deeply personal stories that shaped his life, career, and philosophy. His speech emphasized the importance of trusting your intuition, finding your passion, and embracing life’s challenges with courage and faith.\n",
        "The first story was about connecting the dots. Jobs described how dropping out of college allowed him to take a calligraphy course that seemed useless at the time but later influenced the beautiful typography in the first Macintosh computer. He explained that you can’t connect the dots looking forward—you can only connect them looking backward—so you have to trust that the dots will connect in your future.\n",
        "His second story focused on love and loss.\n",
        "Jobs spoke of how he was fired from Apple, the company he co-founded. Though devastating at first, the experience rekindled his creativity, leading to the founding of NeXT and Pixar. He stressed the importance of loving what you do because that love will keep you going when things get tough.\n",
        "The third story was about death. Jobs shared his brush with a cancer diagnosis and how that experience reminded him that life is short. He urged students to live each day as if it were their last, to not waste time living someone else’s life, and to have the courage to follow their heart and intuition.\n",
        "He ended the speech with the words: \"Stay Hungry. Stay Foolish,\" a motto from the final issue of The Whole Earth Catalog that inspired him throughout his life.\n",
        "Jobs' speech remains a timeless reminder to pursue passion over security, embrace setbacks as opportunities, and to live with purpose and authenticity.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkQb6xmiaJc9",
        "outputId": "551400b6-4ac3-47f0-d55d-755e06e78f98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ng_dwib6cu1d"
      },
      "outputs": [],
      "source": [
        "# stopwords.words('arabic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kdWMT4K-c8NJ"
      },
      "outputs": [],
      "source": [
        "# apply stemming and remve the stop words from para\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2hu_TOXgdJYL"
      },
      "outputs": [],
      "source": [
        "#tokenisation - divide into sentences\n",
        "sentences = nltk.sent_tokenize(para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFZ20D9AdR6U",
        "outputId": "3a4aeb7c-dcf8-44e6-ff94-b9ccc917b9ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MyT2oModNwl",
        "outputId": "33e2af30-7d93-491c-9d4b-d2cabd3771f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In his legendary 2005 commencement speech at Stanford University, Steve Jobs shared three deeply personal stories that shaped his life, career, and philosophy.',\n",
              " 'His speech emphasized the importance of trusting your intuition, finding your passion, and embracing life’s challenges with courage and faith.',\n",
              " 'The first story was about connecting the dots.',\n",
              " 'Jobs described how dropping out of college allowed him to take a calligraphy course that seemed useless at the time but later influenced the beautiful typography in the first Macintosh computer.']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9Gk47k4sdVmK"
      },
      "outputs": [],
      "source": [
        "## Apply stopwords and filter and then apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] #remove stop words\n",
        "  sentences[i] = ' '.join(words)  #concvert all the words into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCrJnHxFgrSz",
        "outputId": "e7c88e63-7b79-48fd-da8d-e40c7eebd563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['in legendari 2005 commenc speech stanford univers , steve job share three deepli person stori shape life , career , philosophi .',\n",
              " 'hi speech emphas import trust intuit , find passion , embrac life ’ challeng courag faith .',\n",
              " 'the first stori connect dot .',\n",
              " 'job describ drop colleg allow take calligraphi cours seem useless time later influenc beauti typographi first macintosh comput .',\n",
              " 'he explain ’ connect dot look forward—y connect look backward—so trust dot connect futur .',\n",
              " 'hi second stori focus love loss .',\n",
              " 'job spoke fire appl , compani co-found .',\n",
              " 'though devast first , experi rekindl creativ , lead found next pixar .',\n",
              " 'he stress import love love keep go thing get tough .',\n",
              " 'the third stori death .',\n",
              " 'job share brush cancer diagnosi experi remind life short .',\n",
              " 'he urg student live day last , wast time live someon els ’ life , courag follow heart intuit .',\n",
              " 'he end speech word : `` stay hungri .',\n",
              " \"stay foolish , '' motto final issu the whole earth catalog inspir throughout life .\",\n",
              " \"job ' speech remain timeless remind pursu passion secur , embrac setback opportun , live purpos authent .\"]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-swVCa-hNoT"
      },
      "source": [
        "### USE SNOW BALL STEMMER\n",
        "for good stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CzStWYaMhgAK"
      },
      "outputs": [],
      "source": [
        "para = \"\"\"In his legendary 2005 commencement speech at Stanford University, Steve Jobs shared three deeply personal stories that shaped his life, career, and philosophy. His speech emphasized the importance of trusting your intuition, finding your passion, and embracing life’s challenges with courage and faith.\n",
        "The first story was about connecting the dots. Jobs described how dropping out of college allowed him to take a calligraphy course that seemed useless at the time but later influenced the beautiful typography in the first Macintosh computer. He explained that you can’t connect the dots looking forward—you can only connect them looking backward—so you have to trust that the dots will connect in your future.\n",
        "His second story focused on love and loss.\n",
        "Jobs spoke of how he was fired from Apple, the company he co-founded. Though devastating at first, the experience rekindled his creativity, leading to the founding of NeXT and Pixar. He stressed the importance of loving what you do because that love will keep you going when things get tough.\n",
        "The third story was about death. Jobs shared his brush with a cancer diagnosis and how that experience reminded him that life is short. He urged students to live each day as if it were their last, to not waste time living someone else’s life, and to have the courage to follow their heart and intuition.\n",
        "He ended the speech with the words: \"Stay Hungry. Stay Foolish,\" a motto from the final issue of The Whole Earth Catalog that inspired him throughout his life.\n",
        "Jobs' speech remains a timeless reminder to pursue passion over security, embrace setbacks as opportunities, and to live with purpose and authenticity.\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0ylylNR5djWr"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3ohKKoNehQ95"
      },
      "outputs": [],
      "source": [
        "## Apply stopwords and filter and then apply Snowball stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] #remove stop words\n",
        "  sentences[i] = ' '.join(words)  #concvert all the words into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFtW37hhhiS_",
        "outputId": "70e86681-8295-4edd-e812-c17bd72281d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['in legendari 2005 commenc speech stanford univers , steve job share three deepli person stori shape life , career , philosophi .',\n",
              " 'his speech emphas import trust intuit , find passion , embrac life ’ challeng courag faith .',\n",
              " 'the first stori connect dot .',\n",
              " 'job describ drop colleg allow take calligraphi cours seem useless time later influenc beauti typographi first macintosh comput .',\n",
              " 'he explain ’ connect dot look forward—you connect look backward—so trust dot connect futur .',\n",
              " 'his second stori focus love loss .',\n",
              " 'job spoke fire appl , compani co-found .',\n",
              " 'though devast first , experi rekindl creativ , lead found next pixar .',\n",
              " 'he stress import love love keep go thing get tough .',\n",
              " 'the third stori death .',\n",
              " 'job share brush cancer diagnosi experi remind life short .',\n",
              " 'he urg student live day last , wast time live someon els ’ life , courag follow heart intuit .',\n",
              " 'he end speech word : `` stay hungri .',\n",
              " \"stay foolish , '' motto final issu the whole earth catalog inspir throughout life .\",\n",
              " \"job ' speech remain timeless remind pursu passion secur , embrac setback opportun , live purpos authent .\"]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpgzgMEiWZJ"
      },
      "source": [
        "# User Lemmetizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0-ruruImiaBr"
      },
      "outputs": [],
      "source": [
        "para = \"\"\"In his legendary 2005 commencement speech at Stanford University, Steve Jobs shared three deeply personal stories that shaped his life, career, and philosophy. His speech emphasized the importance of trusting your intuition, finding your passion, and embracing life’s challenges with courage and faith.\n",
        "The first story was about connecting the dots. Jobs described how dropping out of college allowed him to take a calligraphy course that seemed useless at the time but later influenced the beautiful typography in the first Macintosh computer. He explained that you can’t connect the dots looking forward—you can only connect them looking backward—so you have to trust that the dots will connect in your future.\n",
        "His second story focused on love and loss.\n",
        "Jobs spoke of how he was fired from Apple, the company he co-founded. Though devastating at first, the experience rekindled his creativity, leading to the founding of NeXT and Pixar. He stressed the importance of loving what you do because that love will keep you going when things get tough.\n",
        "The third story was about death. Jobs shared his brush with a cancer diagnosis and how that experience reminded him that life is short. He urged students to live each day as if it were their last, to not waste time living someone else’s life, and to have the courage to follow their heart and intuition.\n",
        "He ended the speech with the words: \"Stay Hungry. Stay Foolish,\" a motto from the final issue of The Whole Earth Catalog that inspired him throughout his life.\n",
        "Jobs' speech remains a timeless reminder to pursue passion over security, embrace setbacks as opportunities, and to live with purpose and authenticity.\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1E702Y1Hiacj"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CB2hhCGIipDn"
      },
      "outputs": [],
      "source": [
        "## Apply stopwords and filter and then apply  WOrdNetLemmetizer\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in set(stopwords.words('english'))] #remove stop words\n",
        "  sentences[i] = ' '.join(words)  #concvert all the words into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZrVaUMWhjKA",
        "outputId": "612fccdf-b312-4bf6-cebc-3285300e0cf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['In legendary 2005 commencement speech Stanford University , Steve Jobs share three deeply personal stories shape life , career , philosophy .',\n",
              " 'His speech emphasize importance trust intuition , find passion , embrace life ’ challenge courage faith .',\n",
              " 'The first story connect dot .',\n",
              " 'Jobs describe drop college allow take calligraphy course seem useless time later influence beautiful typography first Macintosh computer .',\n",
              " 'He explain ’ connect dot look forward—you connect look backward—so trust dot connect future .',\n",
              " 'His second story focus love loss .',\n",
              " 'Jobs speak fire Apple , company co-founded .',\n",
              " 'Though devastate first , experience rekindle creativity , lead found NeXT Pixar .',\n",
              " 'He stress importance love love keep go things get tough .',\n",
              " 'The third story death .',\n",
              " 'Jobs share brush cancer diagnosis experience remind life short .',\n",
              " 'He urge students live day last , waste time live someone else ’ life , courage follow heart intuition .',\n",
              " 'He end speech word : `` Stay Hungry .',\n",
              " \"Stay Foolish , '' motto final issue The Whole Earth Catalog inspire throughout life .\",\n",
              " \"Jobs ' speech remain timeless reminder pursue passion security , embrace setbacks opportunities , live purpose authenticity .\"]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvF465jH7GqU"
      },
      "source": [
        "#Parts of Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuInC5zE8iNs",
        "outputId": "a43e313e-4d9a-4ef2-a0b2-073bcc0b7635"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Qltk0NFtix8H"
      },
      "outputs": [],
      "source": [
        "para = \"\"\"In his legendary 2005 commencement speech at Stanford University, Steve Jobs shared three deeply personal stories that shaped his life, career, and philosophy. His speech emphasized the importance of trusting your intuition, finding your passion, and embracing life’s challenges with courage and faith.\n",
        "The first story was about connecting the dots. Jobs described how dropping out of college allowed him to take a calligraphy course that seemed useless at the time but later influenced the beautiful typography in the first Macintosh computer. He explained that you can’t connect the dots looking forward—you can only connect them looking backward—so you have to trust that the dots will connect in your future.\n",
        "His second story focused on love and loss.\n",
        "Jobs spoke of how he was fired from Apple, the company he co-founded. Though devastating at first, the experience rekindled his creativity, leading to the founding of NeXT and Pixar. He stressed the importance of loving what you do because that love will keep you going when things get tough.\n",
        "The third story was about death. Jobs shared his brush with a cancer diagnosis and how that experience reminded him that life is short. He urged students to live each day as if it were their last, to not waste time living someone else’s life, and to have the courage to follow their heart and intuition.\n",
        "He ended the speech with the words: \"Stay Hungry. Stay Foolish,\" a motto from the final issue of The Whole Earth Catalog that inspired him throughout his life.\n",
        "Jobs' speech remains a timeless reminder to pursue passion over security, embrace setbacks as opportunities, and to live with purpose and authenticity.\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(para)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJSd4VeN7gKR",
        "outputId": "4c35ff55-8e3d-4085-ff2c-45fd753ca4f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKgHwM17w7T"
      },
      "source": [
        "POS Tags\n",
        "\n",
        "| Tag   | Meaning         | Example         |\n",
        "| ----- | --------------- | --------------- |\n",
        "| `NN`  | Noun (Singular) | book, dog       |\n",
        "| `NNS` | Noun (Plural)   | books, dogs     |\n",
        "| `VB`  | Verb (Base)     | go, run         |\n",
        "| `VBD` | Verb (Past)     | went, ran       |\n",
        "| `JJ`  | Adjective       | quick, blue     |\n",
        "| `RB`  | Adverb          | quickly, slowly |\n",
        "| `PRP` | Pronoun         | I, you, they    |\n",
        "| `IN`  | Preposition     | in, on, over    |\n",
        "| `DT`  | Determiner      | the, a, an      |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX47vec37jzW",
        "outputId": "a6eaca04-e447-48cc-e7e6-7a51ed7a6322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('In', 'IN'), ('legendary', 'JJ'), ('2005', 'CD'), ('commencement', 'NN'), ('speech', 'NN'), ('Stanford', 'NNP'), ('University', 'NNP'), (',', ','), ('Steve', 'NNP'), ('Jobs', 'NNP'), ('shared', 'VBD'), ('three', 'CD'), ('deeply', 'NN'), ('personal', 'JJ'), ('stories', 'NNS'), ('shaped', 'VBD'), ('life', 'NN'), (',', ','), ('career', 'NN'), (',', ','), ('philosophy', 'NN'), ('.', '.')]\n",
            "[('His', 'PRP$'), ('speech', 'NN'), ('emphasized', 'VBD'), ('importance', 'NN'), ('trusting', 'VBG'), ('intuition', 'NN'), (',', ','), ('finding', 'VBG'), ('passion', 'NN'), (',', ','), ('embracing', 'VBG'), ('life', 'NN'), ('’', 'NN'), ('challenges', 'VBZ'), ('courage', 'NN'), ('faith', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('first', 'JJ'), ('story', 'NN'), ('connecting', 'VBG'), ('dots', 'NNS'), ('.', '.')]\n",
            "[('Jobs', 'NNP'), ('described', 'VBD'), ('dropping', 'VBG'), ('college', 'NN'), ('allowed', 'VBD'), ('take', 'JJ'), ('calligraphy', 'NN'), ('course', 'NN'), ('seemed', 'VBD'), ('useless', 'JJ'), ('time', 'NN'), ('later', 'RB'), ('influenced', 'VBD'), ('beautiful', 'JJ'), ('typography', 'NN'), ('first', 'RB'), ('Macintosh', 'NNP'), ('computer', 'NN'), ('.', '.')]\n",
            "[('He', 'PRP'), ('explained', 'VBD'), ('’', 'NNP'), ('connect', 'JJ'), ('dots', 'NNS'), ('looking', 'VBG'), ('forward—you', 'NN'), ('connect', 'VBP'), ('looking', 'VBG'), ('backward—so', 'NN'), ('trust', 'NN'), ('dots', 'NNS'), ('connect', 'VBP'), ('future', 'NN'), ('.', '.')]\n",
            "[('His', 'PRP$'), ('second', 'JJ'), ('story', 'NN'), ('focused', 'VBN'), ('love', 'JJ'), ('loss', 'NN'), ('.', '.')]\n",
            "[('Jobs', 'NNP'), ('spoke', 'VBD'), ('fired', 'VBN'), ('Apple', 'NNP'), (',', ','), ('company', 'NN'), ('co-founded', 'JJ'), ('.', '.')]\n",
            "[('Though', 'IN'), ('devastating', 'VBG'), ('first', 'JJ'), (',', ','), ('experience', 'NN'), ('rekindled', 'VBD'), ('creativity', 'NN'), (',', ','), ('leading', 'VBG'), ('founding', 'VBG'), ('NeXT', 'JJ'), ('Pixar', 'NNP'), ('.', '.')]\n",
            "[('He', 'PRP'), ('stressed', 'VBD'), ('importance', 'NN'), ('loving', 'VBG'), ('love', 'VB'), ('keep', 'NN'), ('going', 'VBG'), ('things', 'NNS'), ('get', 'VBP'), ('tough', 'JJ'), ('.', '.')]\n",
            "[('The', 'DT'), ('third', 'JJ'), ('story', 'NN'), ('death', 'NN'), ('.', '.')]\n",
            "[('Jobs', 'NNP'), ('shared', 'VBD'), ('brush', 'NN'), ('cancer', 'NN'), ('diagnosis', 'NN'), ('experience', 'NN'), ('reminded', 'VBD'), ('life', 'NN'), ('short', 'JJ'), ('.', '.')]\n",
            "[('He', 'PRP'), ('urged', 'VBD'), ('students', 'NNS'), ('live', 'JJ'), ('day', 'NN'), ('last', 'JJ'), (',', ','), ('waste', 'NN'), ('time', 'NN'), ('living', 'VBG'), ('someone', 'NN'), ('else', 'RB'), ('’', 'NNP'), ('life', 'NN'), (',', ','), ('courage', 'NN'), ('follow', 'JJ'), ('heart', 'NN'), ('intuition', 'NN'), ('.', '.')]\n",
            "[('He', 'PRP'), ('ended', 'VBD'), ('speech', 'NN'), ('words', 'NNS'), (':', ':'), ('``', '``'), ('Stay', 'NNP'), ('Hungry', 'NNP'), ('.', '.')]\n",
            "[('Stay', 'NNP'), ('Foolish', 'NNP'), (',', ','), (\"''\", \"''\"), ('motto', 'VBZ'), ('final', 'JJ'), ('issue', 'NN'), ('The', 'DT'), ('Whole', 'NNP'), ('Earth', 'NNP'), ('Catalog', 'NNP'), ('inspired', 'VBD'), ('throughout', 'IN'), ('life', 'NN'), ('.', '.')]\n",
            "[('Jobs', 'NNP'), (\"'\", 'POS'), ('speech', 'NN'), ('remains', 'VBZ'), ('timeless', 'JJ'), ('reminder', 'NN'), ('pursue', 'NN'), ('passion', 'NN'), ('security', 'NN'), (',', ','), ('embrace', 'NN'), ('setbacks', 'NNS'), ('opportunities', 'NNS'), (',', ','), ('live', 'VBP'), ('purpose', 'JJ'), ('authenticity', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "# lets find POS Tag\n",
        "for i in range (len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [word for word in words if word not in (stopwords.words('english'))]\n",
        "  # sentences[i] = ' '.join(words)\n",
        "  pos_tag_words = nltk.pos_tag(words)\n",
        "  print(pos_tag_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI3_HTeDAY18"
      },
      "source": [
        "# 9. NAME ENTITY RECOGNITION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wL1k01H8Vz0",
        "outputId": "355fd506-6abf-434e-e754-e422f8651340"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\pkmis\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\words.zip.\n"
          ]
        }
      ],
      "source": [
        "# prompt: write a long sentence for name enitity recognition example. only write a sentence and assign it to sentence variable\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "sentence = \"Barack Obama, the former President of the United States, met with Angela Merkel in Berlin, Germany, to discuss international trade and political stability.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1RECB6bMAoMB",
        "outputId": "6b2778f8-63e0-4a1c-ece0-9235c886a31a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Barack Obama, the former President of the United States, met with Angela Merkel in Berlin, Germany, to discuss international trade and political stability.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94q0cZbs8LEe",
        "outputId": "ee5eb25e-545d-4c52-9547-52644e61bb09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Barack',\n",
              " 'Obama',\n",
              " ',',\n",
              " 'the',\n",
              " 'former',\n",
              " 'President',\n",
              " 'of',\n",
              " 'the',\n",
              " 'United',\n",
              " 'States',\n",
              " ',',\n",
              " 'met',\n",
              " 'with',\n",
              " 'Angela',\n",
              " 'Merkel',\n",
              " 'in',\n",
              " 'Berlin',\n",
              " ',',\n",
              " 'Germany',\n",
              " ',',\n",
              " 'to',\n",
              " 'discuss',\n",
              " 'international',\n",
              " 'trade',\n",
              " 'and',\n",
              " 'political',\n",
              " 'stability',\n",
              " '.']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = nltk.word_tokenize(sentence)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JuZgOMWIA2q0"
      },
      "outputs": [],
      "source": [
        "tag_ele = nltk.pos_tag(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmItMTLY8MJ4",
        "outputId": "9a9bb71d-732f-48ab-8815-d8ca62c20bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (GPE Obama/NNP)\n",
            "  ,/,\n",
            "  the/DT\n",
            "  former/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ,/,\n",
            "  met/VBD\n",
            "  with/IN\n",
            "  (PERSON Angela/NNP Merkel/NNP)\n",
            "  in/IN\n",
            "  (GPE Berlin/NNP)\n",
            "  ,/,\n",
            "  (GPE Germany/NNP)\n",
            "  ,/,\n",
            "  to/TO\n",
            "  discuss/VB\n",
            "  international/JJ\n",
            "  trade/NN\n",
            "  and/CC\n",
            "  political/JJ\n",
            "  stability/NN\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "# nltk.ne_chunk(tag_ele).draw()\n",
        "print(nltk.ne_chunk(tag_ele))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlHERfiKA65Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMhzRdq4eq0JG3OsAQd2eqv",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
